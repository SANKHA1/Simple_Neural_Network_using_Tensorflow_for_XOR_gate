# -*- coding: utf-8 -*-
"""Simple_Neural_Network_using_Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GAAL9o3GaqvajOFc5B4km_qxlrA-WIam
"""

import matplotlib.pyplot as plt
import tensorflow as tf
import numpy as np
import pandas as pd
import sys

tf.__version__

training_data=np.array([[0,0],[0,1],[1,0],[1,1]],"float32")
target_data=np.array([[0],[1],[1],[0]],"float32")

model=tf.keras.models.Sequential()
#Add the layers
model.add(tf.keras.layers.Dense(4,input_dim=2,activation='relu')) #each input sample is on array of length '2' (0,1) , 4= number of outout layers
#it means 2 neurons are spreading to 4 neurons (which are in hidden layer) which is connected to output neuron. For the hidden layer there is no
#constraint, so we use 'Relu' function. Relu = 0 for all (-ve) values, Relu=1 (for all positive values)

model.add(tf.keras.layers.Dense(1,activation='sigmoid')) #Output dimension, here the input dimension is not mentioned since for this layer the 
# input dimension=4 . Since at the ouput we need only '0' or '1' ouput, so we use the sigmoid functuion.

model.compile(loss='mean_squared_error',optimizer='adam',metrics='binary_accuracy') #Here we are configuring the learning process. so that neural
#networks are able to make right adjustments to the weights. Using the weights we need to calculate a number showing how good is our model performing
#or how bad is it performing and then try to get that number lower, that number = 'loss'. Here loss function= Mean Squared Error function. 
#The optimizer rightly optimizes the weights

model.summary() #dense_7=output layer which gives probability score  dense_6 = hidden layer

history=model.fit(training_data,target_data,epochs=500, verbose=2)  #1-> training_data, 2-> target_data, 3-> number of epochs of learning iterations 
#4-> This one tells Tensorflow how much info to print out during the training

print(model.predict(training_data).round()) #-> here we are making predictions
#The predictions are showing -> input (0,0)  output (0)
#                               input (0,1)  output (1)
#                               input (1,0)  output (1)
#                               input (1,1)  output (0)
#You are seeing only the outputs below

loss_curve= history.history["loss"]
acc_curve= history.history["binary_accuracy"]
plt.legend(loc='upper left')
plt.plot(loss_curve,label="Train")
plt.title("Loss")

plt.legend(loc='upper left')
plt.plot(acc_curve,label="Train")
plt.title("Accuracy")
plt.show()